{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy import distance\n",
    "import datetime as datetime\n",
    "from multiprocessing import Process\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datetime_features(input_df, column_name):\n",
    "\n",
    "    input_df[column_name+'_hour'] = input_df[column_name].dt.hour\n",
    "    input_df[column_name+'_minute'] = input_df[column_name].dt.minute\n",
    "    input_df[column_name+'_quarter'] = input_df[column_name].dt.quarter\n",
    "    input_df[column_name+'_month'] = input_df[column_name].dt.month\n",
    "    input_df[column_name+'_year'] = input_df[column_name].dt.year\n",
    "    input_df[column_name+'_week'] = input_df[column_name].dt.isocalendar().week\n",
    "    input_df[column_name+'_day'] = input_df[column_name].dt.day\n",
    "    input_df[column_name+'_dayofweek'] = input_df[column_name].dt.dayofweek\n",
    "\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_geodesic_distance(df_stations, point):\n",
    "    df = df_stations.copy()\n",
    "    for idx, i, j  in zip(df.index, df.station_latitude, df.station_longitude):\n",
    "        df.loc[idx, 'distance'] = distance.distance(point, (i,j)).m\n",
    "        if int(df.loc[idx, 'distance']) == 0:\n",
    "            \n",
    "            return df.loc[idx, \"station_name\"], df.loc[idx, 'distance']\n",
    "    df = df.sort_values(\"distance\")\n",
    "    return df.iloc[0][\"station_name\"], df.iloc[0][\"distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_approximate_stations_locations(df_missing_stations, start_stations, save_file, col =\"start_station\"):\n",
    "    count = 0\n",
    "    for idx, i, j in zip(df_missing_stations.index, \n",
    "                         df_missing_stations[f'{col}_latitude'], \n",
    "                         df_missing_stations[f'{col}_longitude']):\n",
    "        \n",
    "        approx_station, approx_distance = calculate_geodesic_distance(start_stations, \n",
    "                            (i, j))\n",
    "        df_missing_stations.loc[idx,f\"{col}_approx\"] = approx_station\n",
    "        df_missing_stations.loc[idx,f\"{col}_approx_distance\"] = approx_distance\n",
    "        \n",
    "        if count%1000 == 0:\n",
    "            print(count)\n",
    "            df_missing_stations.to_csv(f\"{save_file}\")\n",
    "        count += 1\n",
    "    df_missing_stations.to_csv(f\"{save_file}\")\n",
    "    return df_missing_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_get_approximate_stations_locations(df_missing_stations, start_stations, \n",
    "                            save_file_suffix=\"approx_stations\", batch_size=100000, sleep_time=120):\n",
    "\n",
    "    ranges = np.arange(0, df_missing_stations.shape[0], \n",
    "                       batch_size, dtype=int)\n",
    "    ranges = np.concatenate((ranges, [df_missing_stations.shape[0]]))\n",
    "    x = 0\n",
    "    processes = []\n",
    "    for ii in range(1, len(ranges)):\n",
    "        print(ranges[ii-1], ranges[ii])\n",
    "        save_file = f\"approximate_stations/{save_file_suffix}_{ii}.csv\"\n",
    "        partitioned_df = df_missing_stations.loc[ranges[ii-1]: \n",
    "                                    ranges[ii]].copy().reset_index(drop=True)    \n",
    "        \n",
    "        p = Process(target=get_approximate_stations_locations, \n",
    "                            args = (partitioned_df, start_stations, save_file))\n",
    "        #get_approximate_stations_locations(partitioned_df, start_stations, save_file)\n",
    "        x +=1\n",
    "        p.start()\n",
    "        print(x)\n",
    "        processes.append(p)\n",
    "        time.sleep(sleep_time)\n",
    "    for thread in processes:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3258: DtypeWarning: Columns (31,33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3258: DtypeWarning: Columns (4,5,8,9,35) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_initial = pd.DataFrame()\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "    df_initial = df_initial.append(pd.read_csv(file).rename(columns={\n",
    "        \"started_at\": \"start_time\",\n",
    "        \"ended_at\": \"end_time\",\n",
    "        \"start_lat\": \"start_station_latitude\",\n",
    "        \"start_lng\": \"start_station_longitude\",\n",
    "        \"end_lat\": \"end_station_latitude\",\n",
    "        \"end_lng\": \"end_station_longitude\",\n",
    "        \"member_casual\": \"user_type\"\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial = df_initial.dropna(subset=[\n",
    "    'start_station_latitude',\n",
    "    'start_station_longitude',\n",
    "    'end_station_latitude',\n",
    "    'end_station_longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial['start_time'] = pd.to_datetime(df_initial['start_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial = df_initial[df_initial['start_time'] >= datetime.datetime(2021, 1, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Standard Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stations = df_initial[[\"start_station_name\", \n",
    "                             'start_station_latitude', \n",
    "                             \"start_station_longitude\"]]\\\n",
    ".sort_values(['start_station_latitude', \"start_station_longitude\"])\\\n",
    ".drop_duplicates(subset=['start_station_latitude','start_station_longitude'])\\\n",
    ".dropna().drop_duplicates(subset=['start_station_name'])\\\n",
    ".query(\"start_station_latitude != 0\").reset_index(drop=True).rename(\n",
    "    columns={\"start_station_name\": \"station_name\", \n",
    "            'start_station_latitude': \"station_latitude\",\n",
    "            \"start_station_longitude\": \"station_longitude\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_stations = df_initial[[\"end_station_name\", \n",
    "                           'end_station_latitude', \n",
    "                           \"end_station_longitude\"]]\\\n",
    ".sort_values(['end_station_latitude', \"end_station_longitude\"])\\\n",
    ".drop_duplicates(subset=['end_station_latitude','end_station_longitude'])\\\n",
    ".dropna().drop_duplicates(subset=['end_station_name'])\\\n",
    ".query(\"end_station_latitude != 0\").reset_index(drop=True).rename(\n",
    "    columns={\"end_station_name\": \"station_name\", \n",
    "            'end_station_latitude': \"station_latitude\",\n",
    "            \"end_station_longitude\": \"station_longitude\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_stations = start_stations.append(end_stations).sort_values(\n",
    "    ['station_latitude', \"station_longitude\"]).drop_duplicates(subset=['station_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip duration calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial['start_time'] = pd.to_datetime(df_initial['start_time'])\n",
    "df_initial['end_time'] = pd.to_datetime(df_initial['end_time'])\n",
    "df_initial['duration_sec'] = (df_initial['end_time'] - df_initial['start_time'])/ pd.Timedelta(seconds=1)\n",
    "\n",
    "df_initial = create_datetime_features(df_initial, \"start_time\")\n",
    "df_initial = create_datetime_features(df_initial, \"end_time\")\n",
    "df_initial['user_type'] = df_initial['user_type'].replace({\"member\":\"Subscriber\", \"casual\": \"Customer\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing 2020 data for irrelevancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df_initial.query(\"start_time_year > 2020\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Trips < X minutes duration, where start = end\n",
    "\n",
    "X = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flagging_short_trips(x):\n",
    "    return ((x['end_station_latitude'] == x['start_station_latitude']) &\n",
    "        (x['end_station_longitude'] == x['start_station_longitude'])) \\\n",
    "            | (x['start_station_name'] == x['end_station_name'])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = df_reduced.apply(flagging_short_trips, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_same_stations = df_reduced.loc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minutes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_same_stations = df_reduced.loc[~indexes].append(\n",
    "                        df_same_stations.query(f\"duration_sec < {minutes*60}\")).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_stations = df_no_same_stations[(df_no_same_stations['start_station_name'].isna()) |\\\n",
    "                                          (df_no_same_stations['end_station_name'].isna())].copy()\n",
    "\n",
    "df_existing_stations = df_no_same_stations[~((df_no_same_stations['start_station_name'].isna()) |\\\n",
    "                                             (df_no_same_stations['end_station_name'].isna()))].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookup missing Station Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating missing stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_start_stations_no_duplicates = df_missing_stations.drop_duplicates(\n",
    "                                        subset=['start_station_latitude', 'start_station_longitude'])\n",
    "df_missing_start_stations_no_duplicates = df_missing_start_stations_no_duplicates[\n",
    "                                            df_missing_start_stations_no_duplicates['start_station_name'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_end_stations_no_duplicates = df_missing_stations.drop_duplicates(\n",
    "                                        subset=['end_station_latitude', 'end_station_longitude'])\n",
    "df_missing_end_stations_no_duplicates = df_missing_end_stations_no_duplicates[\n",
    "                                            df_missing_end_stations_no_duplicates['end_station_name'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "approximate_start_stations_df = get_approximate_stations_locations(df_missing_start_stations_no_duplicates, \n",
    "                                   start_stations, save_file=\"approximate_stations/approx_start_stations.csv\", \n",
    "                                                                   col =\"start_station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximate_end_stations_df = get_approximate_stations_locations(df_missing_end_stations_no_duplicates, \n",
    "                                   start_stations, save_file=\"approximate_stations/approx_end_stations.csv\", \n",
    "                                                                 col =\"end_station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximate_start_stations_df = pd.read_csv(\"approximate_stations/approx_start_stations.csv\") \n",
    "approximate_end_stations_df = pd.read_csv(\"approximate_stations/approx_end_stations.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_stations = df_missing_stations.drop(columns=[\n",
    "    'start_station_approx', \n",
    "    'start_station_approx_distance']).merge(\n",
    "    approximate_start_stations_df[[\"start_station_approx\", \n",
    "                             'start_station_latitude',\n",
    "                             'start_station_longitude',\n",
    "                             'start_station_approx_distance']],\n",
    "    on=['start_station_latitude', \n",
    "        'start_station_longitude'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_stations = df_missing_stations.merge(\n",
    "    approximate_end_stations_df[[\"end_station_approx\", \n",
    "                             'end_station_latitude',\n",
    "                             'end_station_longitude',\n",
    "                             'end_station_approx_distance']],\n",
    "    on=['end_station_latitude', \n",
    "        'end_station_longitude'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df_missing_stations['start_station_approx_distance'] < 500\n",
    "df_missing_stations.loc[index, 'start_station_name'] = df_missing_stations.loc[index, 'start_station_approx']\n",
    "\n",
    "index = df_missing_stations['end_station_approx_distance'] < 500\n",
    "df_missing_stations.loc[index, 'end_station_name'] = df_missing_stations.loc[index, 'end_station_approx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completed_stations = df_missing_stations.dropna(subset=['start_station_name', 'end_station_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing_stations = df_existing_stations.drop(columns=['start_station_id', 'end_station_id', 'bike_id', 'rental_access_method', 'Unnamed: 0'])\n",
    "df_completed_stations = df_completed_stations.drop(columns=['start_station_id', 'end_station_id', 'bike_id', 'rental_access_method', 'Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_completed_stations.append(df_existing_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['end_station_name'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"baywheels_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_stations.to_csv(\"standard_stations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = 0\n",
    "#for idx, i, j in zip(df_missing_stations.index, \n",
    "#                     df_missing_stations['start_station_latitude'], \n",
    "#                     df_missing_stations['start_station_longitude']):\n",
    "#    approx_station, approx_distance = calculate_geodesic_distance(start_stations, \n",
    "#                        (i, j))\n",
    "#    df_missing_stations.loc[idx,\"start_station_approx\"] = approx_station\n",
    "#    df_missing_stations.loc[idx,\"start_station_approx_distance\"] = approx_distance\n",
    "#    count += 1\n",
    "#    if count%10000 == 0:\n",
    "#        print(count)\n",
    "#        df_missing_stations.to_csv(\"missing_stations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#import plotly.express as px\n",
    "#fig = px.density_mapbox(df_initial.head(100000), lat='start_station_latitude', lon='start_station_longitude', radius=2,\n",
    "#                        center=dict(lat=0, lon=180), zoom=0,\n",
    "#                        mapbox_style=\"stamen-terrain\")\n",
    "#fig.update_geos(fitbounds=\"locations\")\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics.pairwise import haversine_distances\n",
    "#\n",
    "#points_in_radians = df_initial[['start_station_latitude','start_station_longitude']].head(10000).apply(np.radians).values\n",
    "#distances_in_km = haversine_distances(points_in_radians) * 6371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance_matrix = distances_in_km\n",
    "\n",
    "#clustering = DBSCAN(min_samples=2)\n",
    "#scaler = MinMaxScaler()\n",
    "#df = df_initial.copy()\n",
    "#df[['start_station_latitude', 'start_station_longitude', \n",
    "#    'end_station_latitude', 'end_station_longitude']] = \\\n",
    "#                            scaler.fit_transform(df[['start_station_latitude', 'start_station_longitude', \n",
    "#                                                     'end_station_latitude', 'end_station_longitude']])\n",
    "#clusters = clustering.fit_predict(df[['start_station_latitude', 'start_station_longitude']].head(10000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
